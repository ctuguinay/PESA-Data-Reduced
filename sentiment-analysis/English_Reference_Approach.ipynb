{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oh2QKNTF0nB"
      },
      "outputs": [],
      "source": [
        "# PIP INSTALLATIONS\n",
        "\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4YpB39fiZSvl"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding, pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import gc\n",
        "import evaluate\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f9wBc_uG0Wi"
      },
      "outputs": [],
      "source": [
        "# LOGIN TO WANDB\n",
        "\n",
        "!wandb login\n",
        "\n",
        "%env WANDB_PROJECT=relevance_predictions\n",
        "%env WANDB_LOG_MODEL=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RXjdgMQbZXHF"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "def initialize_data():\n",
        "    data = load_dataset('csv', data_files = \"data_relevance_training.csv\", encoding='utf8')\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def relevance_to_label(relevance):\n",
        "    if relevance == \"relevant\":\n",
        "        return 1\n",
        "    elif relevance == \"irrelevant\":\n",
        "        return 0\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid Relevance Value: {relevance}\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TFWL99qF4sj"
      },
      "outputs": [],
      "source": [
        "# GET DATA\n",
        "\n",
        "data = initialize_data()\n",
        "\n",
        "data = data.map(lambda x: {\"labels\": relevance_to_label(x['relevance'])})\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def encode(training_data):\n",
        "\n",
        "    return tokenizer(training_data['text'], truncation=True, padding='max_length')\n",
        "\n",
        "data = data.map(encode, batched=True)\n",
        "\n",
        "data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "data = data.remove_columns(['relevance', 'type', 'post_id', 'refer_post_id', 'source', 'text', 'date'])\n",
        "datasets = data['train'].train_test_split(test_size = 0.2, train_size = 0.8, seed = 0)\n",
        "training_data = datasets['train']\n",
        "validation_data = datasets['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU6Q6PFkakLZ"
      },
      "outputs": [],
      "source": [
        "# (DELETE LAST MODEL) AND LOAD (NEW) MODEL\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "try:\n",
        "  del model\n",
        "except:\n",
        "  pass\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    model.cuda()\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EPIuAdQbrIjk"
      },
      "outputs": [],
      "source": [
        "# LOAD TRAINER\n",
        "\n",
        "training_args = TrainingArguments(\"ADD EXPERIMENT NAME HERE\",\n",
        "                                  logging_steps=50,\n",
        "                                  optim=\"adamw_torch\",\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  report_to=\"wandb\",\n",
        "                                  load_best_model_at_end=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=training_data,\n",
        "    eval_dataset=validation_data,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg_LmBkWu4k1"
      },
      "outputs": [],
      "source": [
        "# TRAIN\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTKnYxkvHZuq"
      },
      "outputs": [],
      "source": [
        "# POST TRAINING ANALYSIS\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwhvpYltK_5c"
      },
      "outputs": [],
      "source": [
        "# LOAD MODEL\n",
        "\n",
        "# Initialize Wandb Run\n",
        "run = wandb.init()\n",
        "model_artifact = run.use_artifact(\"INSERT ARTIFACT LINK HERE\", type='model')\n",
        "\n",
        "# Download model weights to a folder and return the path\n",
        "model_dir = model_artifact.download()\n",
        "\n",
        "# Load your Hugging Face model from that folder\n",
        "#  using the same model class\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9gl5LuBOh64",
        "outputId": "02d99814-f10e-4cb5-894d-61cbfba78aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'LABEL_1', 'score': 0.9948769211769104}]\n",
            "[{'label': 'LABEL_1', 'score': 0.9943997263908386}]\n",
            "[{'label': 'LABEL_0', 'score': 0.9149153232574463}]\n",
            "[{'label': 'LABEL_0', 'score': 0.9702093005180359}]\n"
          ]
        }
      ],
      "source": [
        "# TEST ON DIFFERENT TEXTS\n",
        "\n",
        "text_classifier = pipeline(task=\"text-classification\", model=loaded_model, tokenizer=loaded_tokenizer)\n",
        "\n",
        "classification = text_classifier(\"Imagine if late president ferdinand marcos sr was still alive and witnessed this glorious moment sneezing . that's the president of the mass sweet and cute dad solid bbm smiling with hearts smiling with hearts smiling with hearts\")\n",
        "\n",
        "print(classification)\n",
        "\n",
        "classification = text_classifier(\"Long live APO UN PBBM Red Heart Red Heart The AFTR shocks are still on the ABRA POEPICENTER STAYSAFE FOLDED HANDS FOLDED HANDS Bangonabrenios . Red Heart\")\n",
        "\n",
        "print(classification)\n",
        "\n",
        "classification = text_classifier(\"The amount of appreciation gratitude and respect he has for the frontliners . Don't Irene Marcos unn seconds turned shade\")\n",
        "\n",
        "print(classification)\n",
        "\n",
        "classification = text_classifier(\"random word\")\n",
        "\n",
        "print(classification)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "English Reference Approach.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "63ff4dc2dd031f4852d607fdb3ec67fe08a87deee7c75aae11edc48254a35360"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
