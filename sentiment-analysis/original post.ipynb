{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates the DataFrame from data_relevance_training.csv and extracts the two needed columns.\n",
    "Then, manipulates the string content in the specified way before saving as df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.7 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/raymo/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import string\n",
    "\n",
    "stops = set(word.lower() for word in list(stopwords.words('english')))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def getPOS(word):\n",
    "    \"\"\"\n",
    "    Gets the part of speech for some word.\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.7 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/raymo/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "ENCODING = {'irrelevant': 0, 'relevant': 1}\n",
    "\n",
    "def processText(text: str) -> str:\n",
    "    text = (text.translate(str.maketrans('', '', string.punctuation))\n",
    "                .split(' . ')[0]\n",
    "                .lower())\n",
    "    split = [w for w in text.split(' ') if (not w in stops and len(w) > 1)]\n",
    "    uniques = ' '.join(sorted(set(split), key=split.index))\n",
    "    lemmatized = []\n",
    "    for w in nltk.word_tokenize(uniques):\n",
    "        lemmatizedW = lemmatizer.lemmatize(w, getPOS(w))\n",
    "        if len(lemmatizedW) > 1:\n",
    "            lemmatized.append(lemmatizedW)\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "data: pd.DataFrame = pd.read_csv('../data/labeled/cleaned/combined/data_relevance_training.csv')\n",
    "encoded: pd.Series = data['relevance'].apply(lambda x: ENCODING[x])\n",
    "\n",
    "text: pd.Series = data['text'].apply(processText)\n",
    "\n",
    "df = pd.DataFrame([text, encoded]).T\n",
    "df = df.astype({'text': 'string', 'relevance': 'int'})\n",
    "print(len(df))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizes the text using sklearn Tfidfvectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2022)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, lowercase=True)\\\n",
    "tf_idf = vectorizer.fit_transform(df['text'])\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tf_idf = normalize(tf_idf).toarray()\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does an 80-20 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tf_idf, df.relevance, test_size= .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates all the classifier models to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier(n_neighbors=5)\n",
    "mnb = MultinomialNB(alpha=0.2)\n",
    "dtc = DecisionTreeClassifier(min_samples_split=2)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=31)\n",
    "ada = AdaBoostClassifier()\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "classifiers = {'SVC': svc, \n",
    "               'KNeighborsClassifier': knc,\n",
    "               'Multinomial Naive Bayes': mnb,\n",
    "               'Decision Tree': dtc,\n",
    "               'Logistic Regression': lrc,\n",
    "               'Random Forest': rfc,\n",
    "               'AdaBoost': ada,\n",
    "               'Multi-Layer Perceptron': mlp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fits and subsequently evaluates the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marko\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\marko\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\marko\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\marko\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\marko\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\marko\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_scores = []\n",
    "for name, model in classifiers.items():\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    train_scores.append((name, accuracy_score(y_test , y_pred)))\n",
    "    \n",
    "crossval_scores = []\n",
    "for name, model in classifiers.items():\n",
    "    scores = cross_val_score(model, x_test, y_test, cv=5)\n",
    "    crossval_scores.append((name, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints the resultant scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Scores\n",
      "SVC: 0.875\n",
      "KNeighborsClassifier: 0.825\n",
      "Multinomial Naive Bayes: 0.88\n",
      "Decision Tree: 0.865\n",
      "Logistic Regression: 0.865\n",
      "Random Forest: 0.865\n",
      "AdaBoost: 0.845\n",
      "Multi-Layer Perceptron: 0.875\n",
      "\n",
      "Cross-Validation Scores\n",
      "SVC: 0.875\n",
      "KNeighborsClassifier: 0.76\n",
      "Multinomial Naive Bayes: 0.8299999999999998\n",
      "Decision Tree: 0.82\n",
      "Logistic Regression: 0.82\n",
      "Random Forest: 0.835\n",
      "AdaBoost: 0.7949999999999999\n",
      "Multi-Layer Perceptron: 0.795\n"
     ]
    }
   ],
   "source": [
    "print('Model Training Scores')\n",
    "for item in train_scores:\n",
    "    print(f'{item[0]}: {item[1]}')\n",
    "    \n",
    "print('\\nCross-Validation Scores')\n",
    "for item in crossval_scores:\n",
    "    print(f'{item[0]}: {item[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests them on the four quotes from the English Reference Approach Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Imagine if late president ferdinand marcos sr was still alive and witnessed this glorious moment sneezing\"\n",
      "\"imagine late president ferdinand marcos sr still alive witness glorious moment sneeze\"\n",
      "\tSVC: Relevant\n",
      "\tKNeighborsClassifier: Relevant\n",
      "\tMultinomial Naive Bayes: Relevant\n",
      "\tDecision Tree: Relevant\n",
      "\tLogistic Regression: Relevant\n",
      "\tRandom Forest: Relevant\n",
      "\tAdaBoost: Relevant\n",
      "\tMulti-Layer Perceptron: Relevant\n",
      "\n",
      "\"Long live APO UN PBBM Red Heart Red Heart The AFTR shocks are still on the ABRA POEPICENTER STAYSAFE FOLDED HANDS FOLDED HANDS Bangonabrenios\"\n",
      "\"long live apo un pbbm red heart aftr shock still abra poepicenter staysafe fold hand bangonabrenios\"\n",
      "\tSVC: Relevant\n",
      "\tKNeighborsClassifier: Relevant\n",
      "\tMultinomial Naive Bayes: Relevant\n",
      "\tDecision Tree: Relevant\n",
      "\tLogistic Regression: Relevant\n",
      "\tRandom Forest: Relevant\n",
      "\tAdaBoost: Relevant\n",
      "\tMulti-Layer Perceptron: Relevant\n",
      "\n",
      "\"The amount of appreciation gratitude and respect he has for the frontliners\"\n",
      "\"amount appreciation gratitude respect frontliners\"\n",
      "\tSVC: Irrelevant\n",
      "\tKNeighborsClassifier: Irrelevant\n",
      "\tMultinomial Naive Bayes: Irrelevant\n",
      "\tDecision Tree: Irrelevant\n",
      "\tLogistic Regression: Irrelevant\n",
      "\tRandom Forest: Irrelevant\n",
      "\tAdaBoost: Irrelevant\n",
      "\tMulti-Layer Perceptron: Irrelevant\n",
      "\n",
      "\"random word\"\n",
      "\"random word\"\n",
      "\tSVC: Irrelevant\n",
      "\tKNeighborsClassifier: Relevant\n",
      "\tMultinomial Naive Bayes: Relevant\n",
      "\tDecision Tree: Irrelevant\n",
      "\tLogistic Regression: Irrelevant\n",
      "\tRandom Forest: Irrelevant\n",
      "\tAdaBoost: Irrelevant\n",
      "\tMulti-Layer Perceptron: Relevant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = [\"Imagine if late president ferdinand marcos sr was still alive and witnessed this glorious moment sneezing\",\n",
    "        \"Long live APO UN PBBM Red Heart Red Heart The AFTR shocks are still on the ABRA POEPICENTER STAYSAFE FOLDED HANDS FOLDED HANDS Bangonabrenios\",\n",
    "        \"The amount of appreciation gratitude and respect he has for the frontliners\",\n",
    "        \"random word\"]\n",
    "\n",
    "DECODING = ['Irrelevant', 'Relevant']\n",
    "\n",
    "for tweet in TEXT:\n",
    "    tweettfidf = vectorizer.transform([processText(tweet)]).toarray()\n",
    "    print(f'\"{tweet}\"')\n",
    "    print(f'\"{processText(tweet)}\"')\n",
    "    for name, model in classifiers.items():\n",
    "        print(f'\\t{name}: {DECODING[model.predict(tweettfidf)[0]]}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63ff4dc2dd031f4852d607fdb3ec67fe08a87deee7c75aae11edc48254a35360"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
